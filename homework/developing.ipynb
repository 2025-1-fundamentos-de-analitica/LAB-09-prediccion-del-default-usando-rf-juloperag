{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8afb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerias\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import  precision_score, balanced_accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a6097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train = pd.read_csv(\n",
    "        \"../files/input/train_data.csv.zip\",\n",
    "        index_col=False,\n",
    "        compression=\"zip\",\n",
    "    )\n",
    "    test = pd.read_csv(\n",
    "        \"../files/input/test_data.csv.zip\",\n",
    "        index_col=False,\n",
    "        compression=\"zip\",\n",
    "    )\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# Paso 1.\n",
    "# Realice la limpieza de los datasets:\n",
    "# - Renombre la columna \"default payment next month\" a \"default\".\n",
    "# - Remueva la columna \"ID\".\n",
    "# - Elimine los registros con informacion no disponible.\n",
    "# - Para la columna EDUCATION, valores > 4 indican niveles superiores\n",
    "#   de educación, agrupe estos valores en la categoría \"others\".\n",
    "def clear_data(df):\n",
    "    #Renombrar\n",
    "    df.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "    #Eliminacion de columna\n",
    "    df = df.drop(\"ID\", axis=1)\n",
    "    #Eliminacion elementos nulos \n",
    "    df.dropna(inplace=True)\n",
    "    #Cambia valores de educacion mayores a 4\n",
    "    df[\"EDUCATION\"] = df[\"EDUCATION\"].apply(lambda x: x if x<=4 else 4)\n",
    "    return df\n",
    "\n",
    "# Paso 2.\n",
    "# Divida los datasets en x_train, y_train, x_test, y_test.\n",
    "def make_train_test_split(df):\n",
    "    #Division en etiquetas \n",
    "    y_df =  df[\"default\"]\n",
    "    #Division en caracteristicas de entrada\n",
    "    x_df = df.drop(\"default\", axis=1)\n",
    "    return x_df, y_df\n",
    "\n",
    "# Paso 3.\n",
    "# Cree un pipeline para el modelo de clasificación. Este pipeline debe\n",
    "# contener las siguientes capas:\n",
    "# - Transforma las variables categoricas usando el método\n",
    "#   one-hot-encoding.\n",
    "# - Ajusta un modelo de bosques aleatorios (rando forest).\n",
    "def estimator_pipeline():\n",
    "    # Define las columnas categóricas\n",
    "    categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "    # Crea el preprocesador\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Deja las columnas numéricas igual\n",
    "    )\n",
    "    #Contruccion pipeline\n",
    "    pipeline = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(),\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# Paso 4.\n",
    "# Optimice los hiperparametros del pipeline usando validación cruzada.\n",
    "# Use 10 splits para la validación cruzada. Use la función de precision\n",
    "# balanceada para medir la precisión del modelo.\n",
    "def cross_validation(estimator, x_train, y_train):\n",
    "    #Hiperparametros a evaluar\n",
    "    #Se debe antemoner el nombre del estimador y luego la lista de valores para un pipeline\n",
    "    param_grid = {\n",
    "        'randomforestclassifier__n_estimators': [75,120,175], #[100, 200],\n",
    "        'randomforestclassifier__max_depth': [None, 5, 10], #| [None, 10, 20], #|\n",
    "        'randomforestclassifier__min_samples_split': [2,5,10], #| [2, 5, 10], #|\n",
    "        'randomforestclassifier__min_samples_leaf': [1,2] #| [1, 2, 4] #|\n",
    "    }\n",
    "    #Evaluacion de hiperparametros\n",
    "    model = GridSearchCV(\n",
    "        estimator= estimator,\n",
    "        param_grid= param_grid,\n",
    "        cv = 10,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "    #Aplicacion de GridSearchCV\n",
    "    model.fit(x_train, y_train)\n",
    "    #Informacion del mejor modelo y ademas definirlo\n",
    "    print(model.best_score_)\n",
    "    print(model.best_params_)\n",
    "    return model\n",
    "\n",
    "# Paso 5.\n",
    "# Guarde el modelo (comprimido con gzip) como \"files/models/model.pkl.gz\".\n",
    "# Recuerde que es posible guardar el modelo comprimido usanzo la libreria gzip.\n",
    "def save_grid_search_model(model):\n",
    "    #Guardar mejor modelo\n",
    "    if not os.path.exists(\"../files/models\"):\n",
    "        os.makedirs(\"../files/models\")\n",
    "    with gzip.open(\"../files/models/model.pkl.gz\", \"wb\") as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "# Paso 6.\n",
    "# Calcule las metricas de precision, precision balanceada, recall,\n",
    "# y f1-score para los conjuntos de entrenamiento y prueba.\n",
    "# Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# Este diccionario tiene un campo para indicar si es el conjunto\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'dataset': 'train', 'precision': 0.8, 'balanced_accuracy': 0.7, 'recall': 0.9, 'f1_score': 0.85}\n",
    "# {'dataset': 'test', 'precision': 0.7, 'balanced_accuracy': 0.6, 'recall': 0.8, 'f1_score': 0.75}       \n",
    "def eval_metrics(type_dataset, y_true, y_pred):\n",
    "    #         | Pronóstico\n",
    "    #         |  PP    PN\n",
    "    #---------|------------\n",
    "    #      P  |  TP    FN\n",
    "    # Real    |\n",
    "    #      N  |  FP    TN\n",
    "\n",
    "    #(1/2)*(TP/P + TN/N)\n",
    "    b_accuracy = balanced_accuracy_score(y_true=y_true, y_pred=y_pred,)\n",
    "    #TP/(TP + FP)\n",
    "    precision = precision_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred, \n",
    "        labels=None, \n",
    "        pos_label=1,\n",
    "        average=\"binary\",)\n",
    "    #TP/(TP + FN)\n",
    "    recall = recall_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred, \n",
    "        labels=None, \n",
    "        pos_label=1,\n",
    "        average=\"binary\",)\n",
    "    #2*(precision*recall)/(precision + recall)\n",
    "    f1 = f1_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        labels=None,\n",
    "        pos_label=1,\n",
    "        average=\"binary\",\n",
    "        sample_weight=None,\n",
    "        zero_division=\"warn\",)\n",
    "\n",
    "    #Formar diccionario de metricas \n",
    "    dic_metrics = { \"type\": \"metrics\",\n",
    "                   'dataset': type_dataset, \n",
    "                   'precision': precision , \n",
    "                   'balanced_accuracy': b_accuracy, \n",
    "                   'recall': recall, \n",
    "                   'f1_score': f1}\n",
    "    print(dic_metrics)\n",
    "    #Guardar metricas como archivo json\n",
    "    if not os.path.exists(\"../files/output\"):\n",
    "        os.makedirs(\"../files/output\")\n",
    "    with open(\"../files/output/metrics.json\", \"a\") as f:\n",
    "        json.dump(dic_metrics, f)\n",
    "        f.write(\"\\n\")\n",
    "# Paso 7.\n",
    "# Calcule las matrices de confusion para los conjuntos de entrenamiento y\n",
    "# prueba. Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'type': 'cm_matrix', 'dataset': 'train', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 666}, 'true_1': {\"predicted_0\": 3333, \"predicted_1\": 1444}}\n",
    "# {'type': 'cm_matrix', 'dataset': 'test', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 650}, 'true_1': {\"predicted_0\": 2490, \"predicted_1\": 1420}}\n",
    "#\n",
    "def eval_confusion_matrix(type_dataset, y_true, y_pred):\n",
    "    #         | Pronóstico\n",
    "    #         |  PP    PN\n",
    "    #---------|------------\n",
    "    #      P  |  TP    FN\n",
    "    # Real    |\n",
    "    #      N  |  FP    TN\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=y_true, y_pred=y_pred,).ravel()\n",
    "\n",
    "    #Formar diccionario de metricas \n",
    "    dic_confusion = {'type': 'cm_matrix', 'dataset': type_dataset, \n",
    "                   'true_0': {\"predicted_0\": int(tn), \"predicte_1\": int(fp)}, \n",
    "                   'true_1': {\"predicted_0\": int(fn), \"predicted_1\": int(tp)}}\n",
    "    print(dic_confusion)\n",
    "    #Guardar metricas como archivo json\n",
    "    if not os.path.exists(\"../files/output\"):\n",
    "        os.makedirs(\"../files/output\")\n",
    "    with open(\"../files/output/metrics.json\", \"a\") as f:\n",
    "        json.dump(dic_confusion, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e9d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de datos\n",
    "train, test = load_data()\n",
    "#Limpieza de datos\n",
    "train = clear_data(train)\n",
    "test = clear_data(test)\n",
    "#Division en etiquetas y caracteristicas de entrada\n",
    "x_train, y_train = make_train_test_split(train)\n",
    "x_test, y_test = make_train_test_split(test)\n",
    "#Creacion de pipelin para el empleo del estimador\n",
    "estimator = estimator_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09bb9515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6588622021527634\n",
      "{'randomforestclassifier__max_depth': None, 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 5, 'randomforestclassifier__n_estimators': 175}\n"
     ]
    }
   ],
   "source": [
    "#Entrenar y establecer mejor modelo deacuerdo a unos hiperparametros establecidos \n",
    "model = cross_validation(estimator, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c06fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'metrics', 'dataset': 'train', 'precision': 0.9975490196078431, 'balanced_accuracy': 0.9732181835396317, 'recall': 0.9471123334038503, 'f1_score': 0.9716766142159523}\n",
      "{'type': 'metrics', 'dataset': 'test', 'precision': 0.653232577665827, 'balanced_accuracy': 0.6746501866515807, 'recall': 0.4075432163436354, 'f1_score': 0.5019354838709678}\n",
      "{'type': 'cm_matrix', 'dataset': 'train', 'true_0': {'predicted_0': 16262, 'predicte_1': 11}, 'true_1': {'predicted_0': 250, 'predicted_1': 4477}}\n",
      "{'type': 'cm_matrix', 'dataset': 'test', 'true_0': {'predicted_0': 6678, 'predicte_1': 413}, 'true_1': {'predicted_0': 1131, 'predicted_1': 778}}\n"
     ]
    }
   ],
   "source": [
    "#Salvar mejor model\n",
    "save_grid_search_model(model)\n",
    "# Calculo de métricas\n",
    "eval_metrics(\"train\", y_train, y_pred=model.best_estimator_.predict(x_train))\n",
    "eval_metrics(\"test\", y_test, y_pred=model.best_estimator_.predict(x_test))\n",
    "# Calculo matriz de confusión\n",
    "eval_confusion_matrix(\"train\", y_train, y_pred=model.best_estimator_.predict(x_train))\n",
    "eval_confusion_matrix(\"test\", y_test, y_pred=model.best_estimator_.predict(x_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mi Entorno Virtual (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
